{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "subsequent-feeling",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# color-blind color scheme\n",
    "plt.style.use('tableau-colorblind10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-motel",
   "metadata": {},
   "source": [
    "Let's make sure that we are using numpy 1.26.4 for backwards compatibility (last version before 2.0, which doesn't yet have support from all packages):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.version.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-evolution",
   "metadata": {
    "tags": []
   },
   "source": [
    "# A simple LIF neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-messaging",
   "metadata": {},
   "source": [
    "We'll define a simple leaky-integrate-and-fire neuron model with leaky integration of a current $I$.\n",
    "\n",
    "This implementation is loosely based on:\n",
    "https://colab.research.google.com/github/johanjan/MOOC-HPFEM-source/blob/master/LIF_ei_balance_irregularity.ipynb#scrollTo=Hhk7e-QreVSh\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-sydney",
   "metadata": {},
   "source": [
    "## LIF dynamcis\n",
    "\n",
    "One neuron follows the LIF dynamcis\n",
    "\n",
    "$C_m \\frac{dv}{dt} = - g_l [v(t) - V_l] + I$,\n",
    "\n",
    "where $v(t)$ is the membrane voltage, $C_m$ is the membrane capactitance, $g_l$ the leak conductance, $V_l$ the leak potential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-inflation",
   "metadata": {},
   "source": [
    "\n",
    "Dividing by $g_l$, we rewrite this more simply as\n",
    "\n",
    "$\\tau_m \\frac{dv}{dt} = - [v(t) - V_l] + I/g_l $, where $\\tau_m \\equiv C_m/g_l $.\n",
    "\n",
    "Because we can only simulate discrete time steps, we estimate the derivative of $v(t)$ by the Euler forward method:\n",
    "\n",
    "$\\frac{dv}{dt} \\mapsto $ `dv / dt` : now, instead of representing the derivative, we have two variables `dv` and `dt`, representing discrete voltage and time differences.\n",
    "\n",
    "__Note that the Euler method carries hidden assumptions, which can break down and distort results.__\n",
    "\n",
    "But for now, we should be fine. The discrete time dynamics we will implement are thus:\n",
    "\n",
    "`dv = {- [v - V_l] + I/g_l } / \\tau_m * dt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-symphony",
   "metadata": {},
   "source": [
    "### Spiking mechanism\n",
    "\n",
    "\n",
    "\n",
    "The spiking mechanism is simply implemented by the condition\n",
    "\n",
    "if $v(t) \\geq V_{th}$, then $v(t + dt) = V_{reset}$.\n",
    "\n",
    "We also implement a refractory period, by adding a counter variable since the last spike. For every time step:\n",
    "\n",
    "- if neuron currently spiking: `refractory_counter` = `tau_ref/dt` (which expresses $\\tau_{ref}$ in terms of time steps)\n",
    "- if neuron is refractory: clamp voltage to `V_{reset}` and decrease `refractory_counter` by one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-taylor",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-awareness",
   "metadata": {},
   "source": [
    "The whole looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIF_neuron:\n",
    "    # initialize a neuron class\n",
    "    # provided parameter dictionary params\n",
    "    def __init__(self, params):\n",
    "        # attach parameters to object\n",
    "        self.V_th, self.V_reset = params['V_th'], params['V_reset']   \n",
    "        self.tau_m, self.g_L = params['tau_m'], params['g_L']        \n",
    "        self.V_init, self.V_L = params['V_init'], params['V_L']       \n",
    "        self.dt = params['dt']\n",
    "        self.tau_ref = params['tau_ref']\n",
    "\n",
    "        # initialize voltage and current\n",
    "        self.v = 0.0\n",
    "        # time steps since last spike\n",
    "        self.refractory_counter = 0\n",
    "    \n",
    "    def LIF_step(self, I):\n",
    "        \"\"\"\n",
    "            Perform one step of the LIF dynamics\n",
    "        \"\"\"\n",
    "        \n",
    "        currently_spiking = False\n",
    "        \n",
    "        if self.refractory_counter > 0:\n",
    "            # if the neuron is still refractory\n",
    "            self.v = self.V_reset\n",
    "            self.refractory_counter = self.refractory_counter - 1\n",
    "        elif self.v >= self.V_th:\n",
    "            # if v is above threshold,\n",
    "            # reset voltage and record spike event\n",
    "            currently_spiking = True\n",
    "            self.v = self.V_reset\n",
    "            self.refractory_counter = self.tau_ref/self.dt\n",
    "        else:\n",
    "            # else, integrate the current:\n",
    "            # calculate the increment of the membrane potential\n",
    "            dv = self.voltage_dynamics(I)\n",
    "            # update the membrane potential\n",
    "            self.v = self.v + dv\n",
    "\n",
    "        return self.v, currently_spiking\n",
    "    \n",
    "    def voltage_dynamics(self, I):\n",
    "        \"\"\"\n",
    "            Calulcates one step of the LI dynamics\n",
    "        \"\"\"\n",
    "        dv = (-(self.v-self.V_L) + I/self.g_L) * (self.dt/self.tau_m)\n",
    "        return dv\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-montreal",
   "metadata": {},
   "source": [
    "You can see that we have split up the functions implementing the dynamics into `LIF_step` and `voltage_dynamics`. We could have put both into the same function, but this will be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "### typical neuron parameters###\n",
    "params['V_th']    = -55. # spike threshold [mV]\n",
    "params['V_reset'] = -75. #reset potential [mV]\n",
    "params['tau_m']   = 10. # membrane time constant [ms]\n",
    "params['g_L']     = 10. #leak conductance [nS]\n",
    "params['V_init']  = -65. # initial potential [mV]\n",
    "params['V_L']     = -75. #leak reversal potential [mV]\n",
    "params['tau_ref']    = 2. # refractory time (ms)\n",
    "params['dt'] = .1  # Simulation time step [ms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize one neuron, i.e. an instance of the class\n",
    "neuron1 = LIF_neuron(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check if everything works by performing one step:\n",
    "neuron1.LIF_step(I=300.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate 500 time steps\n",
    "voltages = []\n",
    "spikes = []\n",
    "for _ in range(500):\n",
    "    v, s = neuron1.LIF_step(I=300.0)\n",
    "    voltages.append(v)\n",
    "    spikes.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(voltages)\n",
    "# for s in np.where(spikes)[0]:\n",
    "#     plt.axvline(s, c='red')\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('V (mV)');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-material",
   "metadata": {},
   "source": [
    "Neat, so we have a single LIF neuron that spikes regularly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-frank",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Overriding classes with __super__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-rapid",
   "metadata": {},
   "source": [
    "Let's say that now we want to implement a different class of neurons: [exponential LIF](https://en.wikipedia.org/wiki/Exponential_integrate-and-fire), which have modified dynamics:\n",
    "\n",
    "$C_m \\frac{dv}{dt} = - g_l [v(t) - V_l - \\Delta_T \\exp\\Big( \\frac{v(t) - V_{trig}}{\\Delta_T} \\Big)] + I$.\n",
    "\n",
    "Instead of firing once $V_{tr}$ is reached, the new term $\\Delta_T \\exp\\Big( \\frac{v(t) - V_{trig}}{\\Delta_T}\\Big)$ starts growing quickly as $v(t)$ approaches $V_{trig}$, modeling a more realistic firing behaviour.\n",
    "\n",
    "So now, we can set $V_{trig}$ to -55 mV, and the old threshold $V_{th}$ has the new function of acting as a separate reset, which we will set to 0 mV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-command",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "To implement this, we could now copy and modify the `LIF_neuron` class. But as models build on eachother, there is a more efficient way of doing this:\n",
    "\n",
    "We can initialize the class `ExpLIF_neuron` as a child of `LIF_neuron`, inheriting all of its attributes and functions. The initialization can be inherited by using the `super()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new class as child of old class\n",
    "class ExpLIF_neuron(LIF_neuron):\n",
    "    def __init__(self, params):\n",
    "        # build on LIF neuron with same settings\n",
    "        # (this will run __init__ of the parent class)\n",
    "        super().__init__(params)\n",
    "        \n",
    "        # we only need to attach additional variables:\n",
    "        self.DeltaT = params['DeltaT']\n",
    "        self.V_exp_trigger = params['V_exp_trigger']\n",
    "    \n",
    "    # now we can just    \n",
    "    def voltage_dynamics(self, I):\n",
    "        \"\"\"\n",
    "            Calulcates one step of the exp-LI dynamics\n",
    "        \"\"\"\n",
    "        dv = (-(self.v-self.V_L) + I/self.g_L + self.DeltaT * np.exp((self.v-self.V_exp_trigger)/self.DeltaT)) * (self.dt/self.tau_m)\n",
    "        return dv\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-absence",
   "metadata": {},
   "source": [
    "So that's why we separated `voltage_dynamics` into its own function: because we knew that we will implement a child class that will modify it, but inherit all other properties:\n",
    "\n",
    "*When you write a class, you want other classes to be able to use it.*\n",
    "*super() makes it easier for other classes to use the class you're writing.*\n",
    "\n",
    "*As Bob Martin says, a good architecture allows you to postpone decision making as long as possible.*\n",
    "\n",
    "*super() can enable that sort of architecture.* [From SO](https://stackoverflow.com/questions/222877/what-does-super-do-in-python-difference-between-super-init-and-expl)\n",
    "\n",
    "(the problem can be that your base class becomes so generic, it is hard to understand what each function does, but that can be fixed by good documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-custody",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional parameters for ExpLIF neurons\n",
    "params['DeltaT'] = 10.0  # sharpness of exponential peak\n",
    "params['V_exp_trigger'] = -55. # threshold for exponential depolarization [mV]\n",
    "params['V_th'] = 0 # new reset threshold [mV]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize one neuron\n",
    "neuron2 = ExpLIF_neuron(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate 500 time steps\n",
    "voltages = []\n",
    "spikes = []\n",
    "for _ in range(500):\n",
    "    v, s = neuron2.LIF_step(I=300.0)\n",
    "    voltages.append(v)\n",
    "    spikes.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(voltages)\n",
    "# for s in np.where(spikes)[0]:\n",
    "#     plt.axvline(s, c='red')\n",
    "# plt.xlim(0, 100)\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('V (mV)');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-weight",
   "metadata": {},
   "source": [
    "Great! So now we have a modified class which reproduces expLIF dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-magnitude",
   "metadata": {},
   "source": [
    "## Adaptive exponential LIF neuron (AdEx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-result",
   "metadata": {},
   "source": [
    "To model biology more closely, we can implement adaptation. This accounts for more diverse neuronal firing patterns, such as adaptation, bursting and initial bursting.\n",
    "\n",
    "Using the same approach as above, implement the following dynamics of AdEX neurons:\n",
    "\n",
    "$C_m \\frac{dv}{dt} = - g_l [v(t) - V_l - \\Delta_T \\exp\\Big( \\frac{v(t) - V_{trig}}{\\Delta_T} \\Big)] + I - \\omega(t)$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-buying",
   "metadata": {},
   "source": [
    "with the adaptation variable $\\omega$. The dynamics of $\\omega$ are determined by two parts:\n",
    "\n",
    "- subthreshold adaptation, which follows $\\tau_\\omega \\frac{d\\omega}{dt} = a [v(t)-V_l] - \\omega$.\n",
    "- a spike-triggered adaptation term, which increases $\\omega$ by $b$ at every spike: if $v(t) \\geq V_{th}$, then $\\omega(t + dt) = \\omega(t) + b$.\n",
    "\n",
    "$\\tau_\\omega$ is the time scale of the adaptation variable; $a$ is the subthreshold adaptation parameter; $b$ is the spike-triggered adaptation parameter.\n",
    "\n",
    "See https://neuronaldynamics.epfl.ch/online/Ch6.S1.html for further explanations on the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement an AdEx neuron class using the ExpLIF parent class\n",
    "class AdEx_neuron(ExpLIF_neuron):\n",
    "    def __init__(self, params):\n",
    "        ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
