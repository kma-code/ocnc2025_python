{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stainless-teach",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# color-blind color scheme\n",
    "plt.style.use('tableau-colorblind10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-hypothetical",
   "metadata": {},
   "source": [
    "Let's make sure that we are using numpy 1.26.4 for backwards compatibility (last version before 2.0, which doesn't yet have support from all packages):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.version.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-partner",
   "metadata": {
    "tags": []
   },
   "source": [
    "# A simple LIF neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-italy",
   "metadata": {},
   "source": [
    "We'll define a simple leaky-integrate-and-fire neuron model with leaky integration of a current $I$.\n",
    "\n",
    "This implementation is loosely based on:\n",
    "https://colab.research.google.com/github/johanjan/MOOC-HPFEM-source/blob/master/LIF_ei_balance_irregularity.ipynb#scrollTo=Hhk7e-QreVSh\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-equality",
   "metadata": {},
   "source": [
    "## LIF dynamcis\n",
    "\n",
    "One neuron follows the LIF dynamcis\n",
    "\n",
    "$C_m \\frac{dv}{dt} = - g_l [v(t) - V_l] + I$,\n",
    "\n",
    "where $v(t)$ is the membrane voltage, $C_m$ is the membrane capactitance, $g_l$ the leak conductance, $V_l$ the leak potential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-second",
   "metadata": {},
   "source": [
    "\n",
    "Dividing by $g_l$, we rewrite this more simply as\n",
    "\n",
    "$\\tau_m \\frac{dv}{dt} = - [v(t) - V_l] + I/g_l $, where $\\tau_m \\equiv C_m/g_l $.\n",
    "\n",
    "Because we can only simulate discrete time steps, we estimate the derivative of $v(t)$ by the Euler forward method:\n",
    "\n",
    "$\\frac{dv}{dt} \\mapsto $ `dv / dt` : now, instead of representing the derivative, we have two variables `dv` and `dt`, representing discrete voltage and time differences.\n",
    "\n",
    "__Note that the Euler method carries hidden assumptions, which can break down and distort results.__\n",
    "\n",
    "But for now, we should be fine. The discrete time dynamics we will implement are thus:\n",
    "\n",
    "`dv = {- [v - V_l] + I/g_l } / \\tau_m * dt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-testing",
   "metadata": {},
   "source": [
    "### Spiking mechanism\n",
    "\n",
    "\n",
    "\n",
    "The spiking mechanism is simply implemented by the condition\n",
    "\n",
    "if $v(t) \\geq V_{th}$, then $v(t + dt) = V_{reset}$.\n",
    "\n",
    "We also implement a refractory period, by adding a counter variable since the last spike. For every time step:\n",
    "\n",
    "- if neuron currently spiking: `refractory_counter` = `tau_ref/dt` (which expresses $\\tau_{ref}$ in terms of time steps)\n",
    "- if neuron is refractory: clamp voltage to `V_{reset}` and decrease `refractory_counter` by one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-cowboy",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-heart",
   "metadata": {},
   "source": [
    "The whole looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIF_neuron:\n",
    "    # initialize a neuron class\n",
    "    # provided parameter dictionary params\n",
    "    def __init__(self, params):\n",
    "        # attach parameters to object\n",
    "        self.V_th, self.V_reset = params['V_th'], params['V_reset']   \n",
    "        self.tau_m, self.g_L = params['tau_m'], params['g_L']        \n",
    "        self.V_init, self.V_L = params['V_init'], params['V_L']       \n",
    "        self.dt = params['dt']\n",
    "        self.tau_ref = params['tau_ref']\n",
    "\n",
    "        # initialize voltage and current\n",
    "        self.v = 0.0\n",
    "        # time steps since last spike\n",
    "        self.refractory_counter = 0\n",
    "    \n",
    "    def LIF_step(self, I):\n",
    "        \"\"\"\n",
    "            Perform one step of the LIF dynamics\n",
    "        \"\"\"\n",
    "        \n",
    "        currently_spiking = False\n",
    "        \n",
    "        if self.refractory_counter > 0:\n",
    "            # if the neuron is still refractory\n",
    "            self.v = self.V_reset\n",
    "            self.refractory_counter = self.refractory_counter - 1\n",
    "        elif self.v >= self.V_th:\n",
    "            # if v is above threshold,\n",
    "            # reset voltage and record spike event\n",
    "            currently_spiking = True\n",
    "            self.v = self.V_reset\n",
    "            self.refractory_counter = self.tau_ref/self.dt\n",
    "        else:\n",
    "            # else, integrate the current:\n",
    "            # calculate the increment of the membrane potential\n",
    "            dv = self.voltage_dynamics(I)\n",
    "            # update the membrane potential\n",
    "            self.v = self.v + dv\n",
    "\n",
    "        return self.v, currently_spiking\n",
    "    \n",
    "    def voltage_dynamics(self, I):\n",
    "        \"\"\"\n",
    "            Calulcates one step of the LI dynamics\n",
    "        \"\"\"\n",
    "        dv = (-(self.v-self.V_L) + I/self.g_L) * (self.dt/self.tau_m)\n",
    "        return dv\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-liberty",
   "metadata": {},
   "source": [
    "You can see that we have split up the functions implementing the dynamics into `LIF_step` and `voltage_dynamics`. We could have put both into the same function, but this will be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "### typical neuron parameters###\n",
    "params['V_th']    = -55. # spike threshold [mV]\n",
    "params['V_reset'] = -75. #reset potential [mV]\n",
    "params['tau_m']   = 10. # membrane time constant [ms]\n",
    "params['g_L']     = 10. #leak conductance [nS]\n",
    "params['V_init']  = -65. # initial potential [mV]\n",
    "params['V_L']     = -75. #leak reversal potential [mV]\n",
    "params['tau_ref']    = 2. # refractory time (ms)\n",
    "params['dt'] = .1  # Simulation time step [ms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize one neuron, i.e. an instance of the class\n",
    "neuron1 = LIF_neuron(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check if everything works by performing one step:\n",
    "neuron1.LIF_step(I=300.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate 500 time steps\n",
    "voltages = []\n",
    "spikes = []\n",
    "for _ in range(500):\n",
    "    v, s = neuron1.LIF_step(I=300.0)\n",
    "    voltages.append(v)\n",
    "    spikes.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(voltages)\n",
    "# for s in np.where(spikes)[0]:\n",
    "#     plt.axvline(s, c='red')\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('V (mV)');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-flower",
   "metadata": {},
   "source": [
    "Neat, so we have a single LIF neuron that spikes regularly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-singapore",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Overriding classes with __super__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-enzyme",
   "metadata": {},
   "source": [
    "Let's say that now we want to implement a different class of neurons: [exponential LIF](https://en.wikipedia.org/wiki/Exponential_integrate-and-fire), which have modified dynamics:\n",
    "\n",
    "$C_m \\frac{dv}{dt} = - g_l [v(t) - V_l - \\Delta_T \\exp\\Big( \\frac{v(t) - V_{trig}}{\\Delta_T} \\Big)] + I$.\n",
    "\n",
    "Instead of firing once $V_{tr}$ is reached, the new term $\\Delta_T \\exp\\Big( \\frac{v(t) - V_{trig}}{\\Delta_T}\\Big)$ starts growing quickly as $v(t)$ approaches $V_{trig}$, modeling a more realistic firing behaviour.\n",
    "\n",
    "So now, we can set $V_{trig}$ to -55 mV, and the old threshold $V_{th}$ has the new function of acting as a separate reset, which we will set to 0 mV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-protein",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "To implement this, we could now copy and modify the `LIF_neuron` class. But as models build on eachother, there is a more efficient way of doing this:\n",
    "\n",
    "We can initialize the class `ExpLIF_neuron` as a child of `LIF_neuron`, inheriting all of its attributes and functions. The initialization can be inherited by using the `super()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new class as child of old class\n",
    "class ExpLIF_neuron(LIF_neuron):\n",
    "    def __init__(self, params):\n",
    "        # build on LIF neuron with same settings\n",
    "        # (this will run __init__ of the parent class)\n",
    "        super().__init__(params)\n",
    "        \n",
    "        # we only need to attach additional variables:\n",
    "        self.DeltaT = params['DeltaT']\n",
    "        self.V_exp_trigger = params['V_exp_trigger']\n",
    "    \n",
    "    # now we can just    \n",
    "    def voltage_dynamics(self, I):\n",
    "        \"\"\"\n",
    "            Calulcates one step of the exp-LI dynamics\n",
    "        \"\"\"\n",
    "        dv = (-(self.v-self.V_L) + I/self.g_L + self.DeltaT * np.exp((self.v-self.V_exp_trigger)/self.DeltaT)) * (self.dt/self.tau_m)\n",
    "        return dv\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-biography",
   "metadata": {},
   "source": [
    "So that's why we separated `voltage_dynamics` into its own function: because we knew that we will implement a child class that will modify it, but inherit all other properties:\n",
    "\n",
    "*When you write a class, you want other classes to be able to use it.*\n",
    "*super() makes it easier for other classes to use the class you're writing.*\n",
    "\n",
    "*As Bob Martin says, a good architecture allows you to postpone decision making as long as possible.*\n",
    "\n",
    "*super() can enable that sort of architecture.* [From SO](https://stackoverflow.com/questions/222877/what-does-super-do-in-python-difference-between-super-init-and-expl)\n",
    "\n",
    "(the problem can be that your base class becomes so generic, it is hard to understand what each function does, but that can be fixed by good documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional parameters for ExpLIF neurons\n",
    "params['DeltaT'] = 10.0  # sharpness of exponential peak\n",
    "params['V_exp_trigger'] = -55. # threshold for exponential depolarization [mV]\n",
    "params['V_th'] = 0 # new reset threshold [mV]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize one neuron\n",
    "neuron2 = ExpLIF_neuron(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate 500 time steps\n",
    "voltages = []\n",
    "spikes = []\n",
    "for _ in range(500):\n",
    "    v, s = neuron2.LIF_step(I=300.0)\n",
    "    voltages.append(v)\n",
    "    spikes.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(voltages)\n",
    "# for s in np.where(spikes)[0]:\n",
    "#     plt.axvline(s, c='red')\n",
    "# plt.xlim(0, 100)\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('V (mV)');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-hostel",
   "metadata": {},
   "source": [
    "Great! So now we have a modified class which reproduces expLIF dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-typing",
   "metadata": {},
   "source": [
    "# Mutability & a common trap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-christmas",
   "metadata": {},
   "source": [
    "Let's say we now dare to simulate two neurons. We could just take our class and set up two instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize two neuron\n",
    "neuron3 = ExpLIF_neuron(params)\n",
    "neuron4 = ExpLIF_neuron(params)\n",
    "\n",
    "# change parameters a little bit to make it more interesting\n",
    "neuron4.DeltaT = 7.0\n",
    "neuron4.V_exp_trigger = -50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-alliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these will now become lists of lists (neurons, time steps)\n",
    "voltages_arr = []\n",
    "spikes_arr = []\n",
    "\n",
    "for neuron in [neuron3, neuron4]:\n",
    "    I = 0.0\n",
    "    voltages = []\n",
    "    spikes = []\n",
    "    for _ in range(500):\n",
    "        v, s = neuron.LIF_step(I=I)\n",
    "        voltages.append(v)\n",
    "        spikes.append(s)\n",
    "        I += 10.0\n",
    "    voltages_arr.append(voltages)\n",
    "    spikes_arr.append(spikes)\n",
    "\n",
    "\n",
    "\n",
    "# I = 0.0\n",
    "# for _ in range(500):\n",
    "#     voltages = [neuron.LIF_step(I=I)[0] for neuron in [neuron3, neuron4]]\n",
    "# #         voltages.append(v)\n",
    "# #         spikes.append(s)\n",
    "#     voltages_arr.append(voltages)\n",
    "#     I += 10.0\n",
    "# #     spikes_arr.append(spikes.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-defensive",
   "metadata": {},
   "outputs": [],
   "source": [
    "voltages_arr = np.array(voltages_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "[neuron.LIF_step(I=300.0) for neuron in [neuron3, neuron4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(voltages_arr.T)\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('V (mV)');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-collaboration",
   "metadata": {},
   "source": [
    "# Extending to populations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-wagon",
   "metadata": {},
   "source": [
    "In the last section, we quickly hacked how to extend from 1 to 2 neurons. But now, say we want to simulate a population of 100 neurons. We could use the same approach as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# population parameters\n",
    "n_neurons = 100\n",
    "\n",
    "population1 = [ExpLIF_neuron(params) for _ in range(n_neurons)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we now have a list of 100 neuron objects:\n",
    "# population1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-ireland",
   "metadata": {},
   "source": [
    "Let's give them all noisy currents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_I, std_I = 300, 100\n",
    "n_steps = 10_000\n",
    "\n",
    "# these will now become lists of lists (neurons, time steps)\n",
    "voltages_arr = []\n",
    "spikes_arr = []\n",
    "\n",
    "for i, neuron in enumerate(population1):\n",
    "    voltages = []\n",
    "    spikes = []\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Working on neuron {i}\")\n",
    "    for _ in range(n_steps):\n",
    "        I = np.random.normal(mean_I, std_I)\n",
    "        v, s = neuron.LIF_step(I=I)\n",
    "        voltages.append(v)\n",
    "        spikes.append(s)\n",
    "    voltages_arr.append(voltages.copy())\n",
    "    spikes_arr.append(spikes.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy array\n",
    "voltages_arr = np.array(voltages_arr)\n",
    "spikes_arr = np.array(spikes_arr)\n",
    "\n",
    "# convert spikes to spike_timings\n",
    "spike_timings = [arr.nonzero()[0] for arr in spikes_arr]\n",
    "\n",
    "for i in range(n_neurons):\n",
    "    x = spike_timings[i]\n",
    "    y = [i for _ in spike_timings[i]]\n",
    "    plt.scatter(x, y, marker='.', c='black')\n",
    "plt.xlim(9000,10000)\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('# Neuron')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-insertion",
   "metadata": {},
   "source": [
    "So it works, but that scales terribly, because every neuron is simulated sequentially.\n",
    "\n",
    "How can we parallelize this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-assumption",
   "metadata": {},
   "source": [
    "## Resisting the urge to hack it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-winning",
   "metadata": {},
   "source": [
    "The essential issue is that `ExpLIF_neuron` only has scalar variables (`v`, `refractory_counter`, ...). We want to extend this to vectors, so that every neuron can be updated in parallel.\n",
    "\n",
    "Having learned about `__super__`, we may be inclined to use it here, right? The issue is that we have baked in the single-neuron property into the class.\n",
    "\n",
    "Remember: \"a good architecture allows you to postpone decision making as long as possible\".\n",
    "\n",
    "So let's do things properly, and use populations as the base of our ExpLIF model.\n",
    "\n",
    "All we need to do is change the scalar variables to vectors. The `if` statements can be neatly taken care of by Numpy indexing and slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpLIF_population:\n",
    "    def __init__(self, params):\n",
    "        # attach parameters to object\n",
    "        self.V_th, self.V_reset = params['V_th'], params['V_reset']   \n",
    "        self.tau_m, self.g_L = params['tau_m'], params['g_L']        \n",
    "        self.V_init, self.V_L = params['V_init'], params['V_L']       \n",
    "        self.dt = params['dt']\n",
    "        self.tau_ref = params['tau_ref']\n",
    "        self.DeltaT = params['DeltaT']\n",
    "        self.V_exp_trigger = params['V_exp_trigger']\n",
    "        \n",
    "        # number of neurons\n",
    "        self.n_neurons = params[\"n_neurons\"]\n",
    "\n",
    "        # initialize voltages\n",
    "        self.v = np.zeros(self.n_neurons)\n",
    "        # time steps since last spike\n",
    "        self.refractory_counter = np.zeros(self.n_neurons)\n",
    "            \n",
    "    def LIF_step(self, I):\n",
    "        \"\"\"\n",
    "            Perform one step of the LIF dynamics\n",
    "        \"\"\"\n",
    "        \n",
    "        currently_spiking = np.array([False for _ in range(self.n_neurons)])\n",
    "        \n",
    "        # This is where the magic happens: numpy indexing.\n",
    "        # first, we need to get indices of neurons which\n",
    "        # are refractory, above threshold or neither:\n",
    "        idx_ref = np.where(self.refractory_counter > 0)[0]\n",
    "        idx_spk = np.where(self.v > self.V_th)[0]\n",
    "        idx_else = np.where((self.refractory_counter <= 0) & (self.v <= self.V_th))[0]\n",
    "        \n",
    "        # if the neuron is still refractory\n",
    "        self.v[idx_ref] = self.V_reset\n",
    "        self.refractory_counter[idx_ref] -= 1\n",
    "        \n",
    "        # if v is above threshold,\n",
    "        # reset voltage and record spike event\n",
    "        currently_spiking[idx_spk] = True\n",
    "        self.v[idx_spk] = self.V_reset\n",
    "        self.refractory_counter[idx_spk] = self.tau_ref/self.dt\n",
    "        \n",
    "        # calculate the increment of the membrane potential\n",
    "        dv = self.voltage_dynamics(I)\n",
    "        # update the membrane potential only for non-spiking neurons\n",
    "        self.v[idx_else] += dv[idx_else]\n",
    "\n",
    "        return self.v, currently_spiking\n",
    "        \n",
    "    def voltage_dynamics(self, I):\n",
    "        \"\"\"\n",
    "            Calulcates one step of the exp-LI dynamics\n",
    "        \"\"\"\n",
    "        # Fortunately, this code already enabled vectors, due to numpy magic.\n",
    "        dv = (-(self.v-self.V_L) + I/self.g_L + self.DeltaT * np.exp((self.v-self.V_exp_trigger)/self.DeltaT)) * (self.dt/self.tau_m)\n",
    "        return dv\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# population parameters\n",
    "n_neurons = 100\n",
    "params[\"n_neurons\"] = n_neurons\n",
    "\n",
    "population2 = ExpLIF_population(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_I, std_I = 300, 100\n",
    "n_steps = 10_000\n",
    "\n",
    "# these will now become lists of lists (neurons, time steps)\n",
    "voltages_arr = []\n",
    "spikes_arr = []\n",
    "\n",
    "for _ in range(n_steps):\n",
    "    I = np.random.normal(mean_I, std_I, size=n_neurons)\n",
    "    v, s = population2.LIF_step(I=I)\n",
    "    voltages_arr.append(v.copy())\n",
    "    spikes_arr.append(s.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy array\n",
    "voltages_arr = np.array(voltages_arr)\n",
    "spikes_arr = np.array(spikes_arr)\n",
    "\n",
    "x_range = (9000,10_000)\n",
    "for i in range(n_neurons):\n",
    "    spike_times = spikes_arr[x_range[0]:x_range[1],i].nonzero()[0]\n",
    "    plt.scatter(spike_times + x_range[0], i*np.ones_like(spike_times), marker='.', c='black')\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('# Neuron')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-bumper",
   "metadata": {},
   "source": [
    "# Timeit and memory allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-cancer",
   "metadata": {},
   "source": [
    "The code we wrote works, but there several things that we can do to make it run faster. Let's also time it using the Jupyter cell magic `%%timeit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"n_neurons\"] = 1_000\n",
    "params[\"n_steps\"] = 10_000\n",
    "\n",
    "mean_I, std_I = 300, 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# population1 = [ExpLIF_neuron(params) for _ in range(n_neurons)]\n",
    "\n",
    "# # these will now become lists of lists (neurons, time steps)\n",
    "# voltages_arr = []\n",
    "# spikes_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n 1 -r 3\n",
    "\n",
    "# for i, neuron in enumerate(population1):\n",
    "#     voltages = []\n",
    "#     spikes = []\n",
    "# #     if i % 10 == 0:\n",
    "# #         print(f\"Working on neuron {i}\")\n",
    "#     for _ in range(params[\"n_steps\"]):\n",
    "#         I = np.random.normal(mean_I, std_I)\n",
    "#         v, s = neuron.LIF_step(I=I)\n",
    "#         voltages.append(v)\n",
    "#         spikes.append(s)\n",
    "#     voltages_arr.append(voltages.copy())\n",
    "#     spikes_arr.append(spikes.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "population2 = ExpLIF_population(params)\n",
    "\n",
    "# these will now become lists of lists (neurons, time steps)\n",
    "voltages_arr = []\n",
    "spikes_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 10\n",
    "\n",
    "for _ in range(params[\"n_steps\"]):\n",
    "    I = np.random.normal(mean_I, std_I, size=params[\"n_neurons\"])\n",
    "    v, s = population2.LIF_step(I=I)\n",
    "    voltages_arr.append(v.copy())\n",
    "    spikes_arr.append(s.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "population3 = ExpLIF_population(params)\n",
    "\n",
    "# these will now become lists of lists (neurons, time steps)\n",
    "voltages_arr = np.zeros((params[\"n_steps\"], params[\"n_neurons\"]))\n",
    "spikes_arr = np.zeros((params[\"n_steps\"], params[\"n_neurons\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-program",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 10\n",
    "\n",
    "for i in range(params[\"n_steps\"]):\n",
    "    I = np.random.normal(mean_I, std_I, size=params[\"n_neurons\"])\n",
    "    voltages_arr[i], spikes_arr[i] = population3.LIF_step(I=I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-hobby",
   "metadata": {},
   "source": [
    "Let's look at some spike rasters and a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = (9000,10_000)\n",
    "for i in range(n_neurons):\n",
    "    spike_times = spikes_arr[x_range[0]:x_range[1],i].nonzero()[0]\n",
    "    plt.scatter(spike_times + x_range[0], i*np.ones_like(spike_times), marker='.', c='black')\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('# Neuron')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-puzzle",
   "metadata": {},
   "source": [
    "# Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-houston",
   "metadata": {},
   "source": [
    "So far, we are using numpy in its simplest form: a single process running on CPU. We can take advantage of multicore systems by using multiprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess as mp   # for multiprocessing\n",
    "\n",
    "# declare the number of processes to start\n",
    "N_PROCESSES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-importance",
   "metadata": {},
   "source": [
    "As a simple example, let's use the squaring of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(N_PROCESSES) as pool:\n",
    "    output = pool.map(f, [1, 2, 3])\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-world",
   "metadata": {},
   "source": [
    "Great, that works. But what if we want to have a more general f(x), like being able to choose the exponent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x, n):\n",
    "    return x**n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-prize",
   "metadata": {},
   "source": [
    "Let's calulcate $1^3, 2^3, 3^3$, i.e. $n=3$ for all cases. In the above example, this could be our parameter set `params`.\n",
    "\n",
    "We might assume that we can pass a tuple or a list, but this fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(N_PROCESSES) as pool:\n",
    "    output = pool.map(g, [(1,2), (2,3), (3,3)])\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-justice",
   "metadata": {},
   "source": [
    "Instead, we need to wrap our function into a partial function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "partial_run = functools.partial(g, n=3) # this instantiates a copy of g with one argument less\n",
    "partial_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(N_PROCESSES) as pool:\n",
    "    output = pool.map(partial_run, [1, 2, 3])\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-overview",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
